{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of LDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A8jOJ951B23",
        "colab_type": "code",
        "outputId": "365c63af-a7dd-45a6-aae1-fd7920bdfced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPi4vFnK1HoA",
        "colab_type": "code",
        "outputId": "4bb57ee9-056a-4896-d602-cd027c489cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import pandas as pd\n",
        "#import numpy as np\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkGystvt1gvT",
        "colab_type": "code",
        "outputId": "220572ac-c110-4ee1-94fc-8ff465051a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfD-YdT31k8O",
        "colab_type": "code",
        "outputId": "4743e31a-7b60-4ed0-d977-499b70f21538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd drive/My Drive/RecommenderSystems And TopicModelling Lab2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RecommenderSystems And TopicModelling Lab2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpcBbFwI2CAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"data/\"\n",
        "news_path=path+\"nytimes_news_articles.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W6Bi3ta2EHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords=np.load(path+'stopwords.npy')\n",
        "news_df = pd.read_pickle(path+'news_df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqYj-sO1BfPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#news_df=news_df.truncate(before=0,after=2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w07-yjfV2JH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq_map={}\n",
        "ps=PorterStemmer()\n",
        "for article in news_df['Article']:\n",
        "    word_list=word_tokenize(article)\n",
        "    article_map={}\n",
        "    for word in word_list:\n",
        "        word=word.replace('\\n','').replace('-',' ')\n",
        "        if len(word)>2:\n",
        "            if word.isalpha():\n",
        "                if word not in stopwords:\n",
        "                    word_s=ps.stem(word)\n",
        "                    if word_s not in stopwords:\n",
        "                      try :\n",
        "                        float(word_s.replace(',',''))\n",
        "                        continue\n",
        "                      except :  \n",
        "                        if word_s not in article_map:\n",
        "                            if word_s in freq_map:\n",
        "                                freq_map[word_s]+=1\n",
        "                            else:\n",
        "                                freq_map[word_s]=1\n",
        "                            article_map[word_s]=1    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K5APzvz-4np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index={}\n",
        "index_to_word={}\n",
        "index=0\n",
        "for key in freq_map:\n",
        "  word_to_index[key]=index\n",
        "  index_to_word[index]=key\n",
        "  index+=1  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G_Q02JnG2Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articlez=[]\n",
        "ps=PorterStemmer()\n",
        "for article in news_df['Article']:\n",
        "    this_article=[]\n",
        "    word_list=word_tokenize(article)\n",
        "    #article_map={}\n",
        "    for word in word_list:\n",
        "        word=word.replace('\\n','').replace('-',' ')\n",
        "        if len(word)>2:\n",
        "            if word.isalpha():\n",
        "                if word not in stopwords:\n",
        "                    word_s=ps.stem(word)\n",
        "                    if word_s not in stopwords:\n",
        "                      try :\n",
        "                        float(word_s.replace(',',''))\n",
        "                        continue\n",
        "                      except :  \n",
        "                        this_article.append(word_to_index[word_s])\n",
        "    articlez.append(this_article)                        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYp9vuKxIg2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(articlez)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9P6R0ZZ2Q85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_articles=len(news_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvArFyooCV8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_terms=len(freq_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxz3DlWX2Tuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_topics=70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw7uhm5k2VfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkgnohJk2YiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector_space_table=np.load(path+'vector_space_table2.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6WEnHr8CQix",
        "colab_type": "code",
        "outputId": "e9fddac9-3efe-494a-9f67-4541df45c353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vector_space_table.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8888, 68953)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY-yBZ8rBxtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(number_of_articles):\n",
        "#   article=news_df['Article'][i]\n",
        "#   word_list=word_tokenize(article)\n",
        "#   article_map={}\n",
        "#   for word in word_list:\n",
        "#     word=word.replace('\\n','').replace('-',' ')\n",
        "#     if len(word)>2:\n",
        "#       if word.isalpha():\n",
        "#         if word not in stopwords:\n",
        "#           word_s=ps.stem(word)\n",
        "#           if word_s not in stopwords:\n",
        "#             try :\n",
        "#               float(word_s.replace(',',''))\n",
        "#               continue\n",
        "#             except :  \n",
        "#               if word_s in article_map:\n",
        "#                 article_map[word_s]+=1\n",
        "#               else:\n",
        "#                 article_map[word_s]=1\n",
        "#   for word in article_map:\n",
        "#     map_index=word_to_index[word]\n",
        "#     freq_art=article_map[word]\n",
        "#     freq_total=freq_map[word]\n",
        "#     vector_space_table[i][map_index]=freq_art"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDZY6OXv2bQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neta=0.1\n",
        "alpha=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvdmAV6b3WIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iters=100\n",
        "vec_table=vector_space_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wST9zvN3XgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_docs=len(vec_table[:,0])\n",
        "number_of_terms=len(vec_table[0])\n",
        "# word_topic_mat=np.zeros(shape=(number_of_topics,number_of_terms))+neta\n",
        "# doc_topic_mat=np.zeros(shape=(number_of_docs,number_of_topics)) +alpha\n",
        "# topix=[]\n",
        "# num_topics=np.zeros(number_of_topics)+number_of_terms*neta\n",
        "# for i in range(number_of_docs):\n",
        "#   topix_cur_doc=[]\n",
        "#   for j in articlez[i]:\n",
        "#     pz=np.divide(np.multiply(doc_topic_mat[i,:],word_topic_mat[:,j]),num_topics)\n",
        "#     z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
        "#     topix_cur_doc.append(z)\n",
        "#     #topic=topic_mat[i][j]\n",
        "#     topic=z\n",
        "#     doc_topic_mat[i][topic]+=1\n",
        "#     word_topic_mat[topic][j]+=1\n",
        "#     num_topics[topic]+=1\n",
        "#   topix.append(topix_cur_doc)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbD1C2CdJfvX",
        "colab_type": "code",
        "outputId": "a2419d3c-58c1-4980-edbd-a44e033abba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "doc_topic_mat[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.1,  0.1,  0.1,  0.1,  0.1,  4.1,  1.1, 39.1,  1.1,  0.1,  0.1,\n",
              "        0.1,  0.1, 13.1,  0.1,  4.1,  0.1,  0.1,  0.1,  0.1,  7.1,  3.1,\n",
              "        0.1, 10.1,  0.1, 72.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  1.1,\n",
              "        0.1,  0.1,  3.1,  1.1,  1.1,  0.1,  0.1,  0.1, 30.1,  0.1,  0.1,\n",
              "        0.1,  0.1,  0.1,  0.1, 79.1,  0.1,  0.1,  0.1,  0.1,  0.1, 54.1,\n",
              "       40.1,  0.1,  0.1, 18.1,  0.1,  0.1,  6.1,  0.1, 18.1,  0.1,  0.1,\n",
              "        0.1,  0.1,  0.1,  1.1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlpYlJMeCpEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_lda():\n",
        "  for d in range(number_of_docs):\n",
        "    if(d%1500==0 and d!=0):\n",
        "      print(d)\n",
        "    index=0\n",
        "    for w in articlez[d]:\n",
        "      z=topix[d][index]\n",
        "      word_topic_mat[z][w]-=1\n",
        "      doc_topic_mat[d][z]-=1\n",
        "      num_topics[z]-=1\n",
        "      pz = np.divide(np.multiply(doc_topic_mat[d, :], word_topic_mat[:, w]), num_topics)\n",
        "      z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
        "      topix[d][index]=z\n",
        "      word_topic_mat[z][w]+=1\n",
        "      doc_topic_mat[d][z]+=1\n",
        "      num_topics[z]+=1            \n",
        "      index+=1\n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og8pnVv-EOGB",
        "colab_type": "code",
        "outputId": "eb18faf4-12d9-4d42-cd1b-5dc4ccd9cd2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "doc_topic_mat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
              "       ...,\n",
              "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suSyzfpYD4FH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(26):\n",
        "  print(\"Iter Num: \"+str(i+1))\n",
        "  new_lda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e5Ib5Wt_3IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save(path+'topic_mat',topic_mat)\n",
        "# np.save(path+'doc_topic',doc_topic_mat)\n",
        "# np.save(path+'word_topic',word_topic_mat) \n",
        "doc_word_map={}\n",
        "for i in range(number_of_docs):\n",
        "  doc_word_map[i]=[]\n",
        "  for j in range(number_of_terms):\n",
        "    if(vec_table[i][j]>0):\n",
        "      doc_word_map[i].append(j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH95as7H82qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_array=np.zeros(number_of_terms)\n",
        "for i in range(number_of_terms):\n",
        "  sum_array[i]=sum(vec_table[:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwhRa2r79NxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_termss=[]\n",
        "for i in range(number_of_docs):\n",
        "  number_of_termss.append(sum(vec_table[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjPAVVV14ROJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lda_algo():\n",
        "  for i in range(number_of_docs):\n",
        "    if(i%1500==0 and i !=0):\n",
        "      print(i)\n",
        "    num_of_terms=number_of_termss[i]\n",
        "    for j in doc_word_map[i]:\n",
        "      p_final=0\n",
        "      init_topic=topic_mat[i][j]\n",
        "      topic_final=topic_mat[i][j]\n",
        "      for topic in range(number_of_topics):\n",
        "        #topic=topic_mat[i][j]\n",
        "        num_terms=num_of_terms-vec_table[i][j]+number_of_topics*alpha\n",
        "        num_terms_assigned=doc_topic_mat[i][topic]-vec_table[i][j]+alpha\n",
        "        if(num_terms!=0):\n",
        "          p1=num_terms_assigned/num_terms\n",
        "        else:\n",
        "          p1=0\n",
        "          \n",
        "        num_term_in_all_docs=sum_array[j]-vec_table[i][j]+number_of_terms*neta\n",
        "        num_term_with_this_topic=word_topic_mat[topic][j]-vec_table[i][j]+neta\n",
        "        if(num_term_in_all_docs!=0):\n",
        "          p2=num_term_with_this_topic/num_term_in_all_docs\n",
        "        else:    \n",
        "          p2=0\n",
        "        p_final1=p1*p2\n",
        "        if(p_final1>p_final):\n",
        "          p_final=p_final1\n",
        "          topic_final=topic\n",
        "      if init_topic!=topic_final:    \n",
        "        word_topic_mat[init_topic][j]-=  vec_table[i][j]\n",
        "        doc_topic_mat[i][init_topic]-= vec_table[i][j]\n",
        "        word_topic_mat[topic_final][j]+=  vec_table[i][j]\n",
        "        doc_topic_mat[i][topic_final]+= vec_table[i][j] \n",
        "        topic_mat[i][j]=topic_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rif0M42AErqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixhq0cxT_1ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(iters):\n",
        "  print(\"Iter Num \"+str(i+1))\n",
        "  lda_algo()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRrvQndGwD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_topic_mat=np.load(path+'word_topic_mat.npy')\n",
        "doc_topic_mat=np.load(path+'doc_topic.npy')\n",
        "topix=np.load(path+'topix.npy') \n",
        "num_topics=np.load(path+'num_topics.npy') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d401CEpWCEU_",
        "colab_type": "code",
        "outputId": "4bb6731d-05f8-406a-f64b-8b6d5b317274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for k in range(number_of_topics):\n",
        "  print(\"Topic \"+ str(k+1)+\" :\")\n",
        "  ids=word_topic_mat[k,:].argsort()\n",
        "  for j in range (1,10):\n",
        "    print(index_to_word[ids[-j]]+\", \",end='')\n",
        "  print('\\n')  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1 :\n",
            "investig, compani, report, file, case, lawyer, lawsuit, email, depart, \n",
            "\n",
            "Topic 2 :\n",
            "art, work, museum, artist, exhibit, galleri, show, new, paint, \n",
            "\n",
            "Topic 3 :\n",
            "percent, price, rose, oil, stock, fell, market, energi, cent, \n",
            "\n",
            "Topic 4 :\n",
            "travel, hotel, trip, airport, guest, includ, visit, park, book, \n",
            "\n",
            "Topic 5 :\n",
            "film, music, movi, song, album, perform, play, festiv, new, \n",
            "\n",
            "Topic 6 :\n",
            "game, point, said, play, warrior, curri, final, player, jame, \n",
            "\n",
            "Topic 7 :\n",
            "plane, fire, flight, air, egyptian, drone, aircraft, canada, fort, \n",
            "\n",
            "Topic 8 :\n",
            "anderson, cox, nativ, medit, ami, pugh, farc, duncan, wallac, \n",
            "\n",
            "Topic 9 :\n",
            "gay, transgend, right, peopl, bathroom, bar, lesbian, gender, north, \n",
            "\n",
            "Topic 10 :\n",
            "sherman, toast, ark, duckpin, ticketmast, norwegian, bowl, oslo, twain, \n",
            "\n",
            "Topic 11 :\n",
            "food, restaur, cook, recip, chef, can, make, chicken, eat, \n",
            "\n",
            "Topic 12 :\n",
            "music, opera, orchestra, compos, concert, hall, bitcoin, met, perform, \n",
            "\n",
            "Topic 13 :\n",
            "dog, puerto, rico, cat, rican, pet, anim, island, tillman, \n",
            "\n",
            "Topic 14 :\n",
            "tribun, publish, gannett, earthquak, ecuador, newspap, mcdonald, ferro, mitchel, \n",
            "\n",
            "Topic 15 :\n",
            "bank, financi, rate, tax, money, million, economi, econom, market, \n",
            "\n",
            "Topic 16 :\n",
            "million, auction, sale, sold, market, price, christi, dealer, buyer, \n",
            "\n",
            "Topic 17 :\n",
            "china, chines, beij, hong, kong, parti, taiwan, govern, foreign, \n",
            "\n",
            "Topic 18 :\n",
            "state, said, govern, unit, attack, militari, islam, american, offici, \n",
            "\n",
            "Topic 19 :\n",
            "skin, hair, bodi, product, beauti, makeup, pierc, vitamin, perfum, \n",
            "\n",
            "Topic 20 :\n",
            "said, year, will, new, one, also, can, state, peopl, \n",
            "\n",
            "Topic 21 :\n",
            "facebook, compani, appl, onlin, googl, technolog, app, media, use, \n",
            "\n",
            "Topic 22 :\n",
            "drug, health, medic, patient, hospit, care, doctor, insur, treatment, \n",
            "\n",
            "Topic 23 :\n",
            "said, offic, case, polic, court, charg, judg, lawyer, prosecutor, \n",
            "\n",
            "Topic 24 :\n",
            "trump, clinton, republican, campaign, democrat, parti, said, sander, state, \n",
            "\n",
            "Topic 25 :\n",
            "princ, baker, minneapoli, willerslev, estat, musician, paisley, sri, purpl, \n",
            "\n",
            "Topic 26 :\n",
            "game, run, said, hit, yanke, met, inning, pitch, season, \n",
            "\n",
            "Topic 27 :\n",
            "citi, new, said, york, mayor, offici, blasio, depart, peopl, \n",
            "\n",
            "Topic 28 :\n",
            "season, throne, jon, snow, game, ramsay, cersei, dragon, aravena, \n",
            "\n",
            "Topic 29 :\n",
            "new, york, graduat, univers, father, coupl, marri, receiv, mother, \n",
            "\n",
            "Topic 30 :\n",
            "design, fashion, wear, brand, dress, collect, cloth, show, men, \n",
            "\n",
            "Topic 31 :\n",
            "hastert, bedroom, room, pool, dutert, properti, philippin, bathroom, hous, \n",
            "\n",
            "Topic 32 :\n",
            "player, team, play, coach, leagu, season, game, year, said, \n",
            "\n",
            "Topic 33 :\n",
            "sex, panama, prostitut, der, buffett, worker, mossack, fonseca, offshor, \n",
            "\n",
            "Topic 34 :\n",
            "space, nasa, light, planet, earth, hole, moon, black, sun, \n",
            "\n",
            "Topic 35 :\n",
            "brazil, rousseff, presid, brazilian, parti, countri, polit, impeach, temer, \n",
            "\n",
            "Topic 36 :\n",
            "car, driver, vehicl, volkswagen, recal, test, drive, safeti, compani, \n",
            "\n",
            "Topic 37 :\n",
            "iran, iranian, rhode, nuclear, saudi, sanction, agreement, deal, tehran, \n",
            "\n",
            "Topic 38 :\n",
            "build, citi, hous, street, new, home, said, park, space, \n",
            "\n",
            "Topic 39 :\n",
            "theater, show, play, broadway, will, music, perform, award, new, \n",
            "\n",
            "Topic 40 :\n",
            "like, one, can, just, time, said, make, get, way, \n",
            "\n",
            "Topic 41 :\n",
            "said, one, famili, peopl, day, year, live, friend, time, \n",
            "\n",
            "Topic 42 :\n",
            "danc, ballet, wine, dancer, choreograph, compani, grape, vineyard, jone, \n",
            "\n",
            "Topic 43 :\n",
            "north, korea, japan, south, nuclear, obama, korean, kim, japanes, \n",
            "\n",
            "Topic 44 :\n",
            "하지만, 말했다, 것이다, falun, gong, 레이저, kasher, 때문에, leggero, \n",
            "\n",
            "Topic 45 :\n",
            "jackson, beer, hornacek, spear, drink, knick, anthoni, softe, brew, \n",
            "\n",
            "Topic 46 :\n",
            "darti, fnac, circu, conforama, peterson, wiltshir, steinhoff, hobb, nash, \n",
            "\n",
            "Topic 47 :\n",
            "compani, busi, billion, million, execut, year, invest, deal, market, \n",
            "\n",
            "Topic 48 :\n",
            "water, plant, climat, energi, environment, power, electr, product, industri, \n",
            "\n",
            "Topic 49 :\n",
            "robot, hadid, ozick, böhmermann, seneg, polanski, cashin, dakar, zaha, \n",
            "\n",
            "Topic 50 :\n",
            "team, game, player, soccer, play, goal, leagu, fan, club, \n",
            "\n",
            "Topic 51 :\n",
            "state, law, court, senat, democrat, republican, hous, bill, justic, \n",
            "\n",
            "Topic 52 :\n",
            "redston, viacom, hors, race, derbi, dauman, nyquist, amus, director, \n",
            "\n",
            "Topic 53 :\n",
            "book, women, time, articl, new, stori, write, read, wrote, \n",
            "\n",
            "Topic 54 :\n",
            "luke, kesha, opera, graham, mose, soni, haywood, abballa, widad, \n",
            "\n",
            "Topic 55 :\n",
            "rugbi, zappa, zealand, rudolph, schoning, australia, africa, dweezil, abdeslam, \n",
            "\n",
            "Topic 56 :\n",
            "show, season, televis, seri, watch, network, episod, netflix, comedi, \n",
            "\n",
            "Topic 57 :\n",
            "jewish, tiger, jew, templ, rabbi, mosqu, muslim, religi, kosher, \n",
            "\n",
            "Topic 58 :\n",
            "ali, muhammad, fight, box, ring, clay, frazier, champion, sport, \n",
            "\n",
            "Topic 59 :\n",
            "research, studi, zika, diseas, scientist, health, viru, found, brain, \n",
            "\n",
            "Topic 60 :\n",
            "india, cuban, indian, cuba, pitsiladi, kenya, german, berlin, modi, \n",
            "\n",
            "Topic 61 :\n",
            "anim, bird, speci, zoo, wildlif, gorilla, park, eleph, conserv, \n",
            "\n",
            "Topic 62 :\n",
            "school, student, univers, colleg, educ, children, class, graduat, black, \n",
            "\n",
            "Topic 63 :\n",
            "european, union, britain, europ, vote, british, parti, countri, leav, \n",
            "\n",
            "Topic 64 :\n",
            "olymp, russia, russian, athlet, sport, dope, rio, game, world, \n",
            "\n",
            "Topic 65 :\n",
            "allen, nepal, climb, politico, romeo, nepales, panton, juliet, cervant, \n",
            "\n",
            "Topic 66 :\n",
            "israel, isra, palestinian, gaza, tunnel, netanyahu, jerusalem, hama, bank, \n",
            "\n",
            "Topic 67 :\n",
            "church, cancer, christian, cloud, cathol, tumor, cell, andrew, pope, \n",
            "\n",
            "Topic 68 :\n",
            "game, goal, team, score, first, seri, second, play, said, \n",
            "\n",
            "Topic 69 :\n",
            "polic, shoot, gun, kill, orlando, shot, attack, peopl, victim, \n",
            "\n",
            "Topic 70 :\n",
            "island, sea, boat, water, beach, ship, ocean, fish, coast, \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKeuWJnHEiZx",
        "colab_type": "code",
        "outputId": "09c2b831-e848-46ed-e04b-463180968f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "niters=100\n",
        "vals=[0.1]\n",
        "for neta in vals:\n",
        "  for alpha in vals:\n",
        "    print(alpha)\n",
        "    print(neta)\n",
        "    number_of_docs=len(vec_table[:,0])\n",
        "    number_of_terms=len(vec_table[0])\n",
        "    word_topic_mat=np.zeros(shape=(number_of_topics,number_of_terms))+neta\n",
        "    doc_topic_mat=np.zeros(shape=(number_of_docs,number_of_topics)) +alpha\n",
        "    topix=[]\n",
        "    num_topics=np.zeros(number_of_topics)+number_of_terms*neta\n",
        "    for i in range(number_of_docs):\n",
        "      topix_cur_doc=[]\n",
        "      for j in articlez[i]:\n",
        "        pz=np.divide(np.multiply(doc_topic_mat[i,:],word_topic_mat[:,j]),num_topics)\n",
        "        z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
        "        topix_cur_doc.append(z)\n",
        "        #topic=topic_mat[i][j]\n",
        "        topic=z\n",
        "        doc_topic_mat[i][topic]+=1\n",
        "        word_topic_mat[topic][j]+=1\n",
        "        num_topics[topic]+=1\n",
        "      topix.append(topix_cur_doc) \n",
        "    for i in range(iters):\n",
        "      \n",
        "      print(\"Iter Num \"+str(i+1))\n",
        "      new_lda()\n",
        "      if(iters%10==0):\n",
        "        np.save(path+'word_topic_mat',word_topic_mat)\n",
        "        np.save(path+'doc_topic',doc_topic_mat)\n",
        "        np.save(path+'topix',topix) \n",
        "        np.save(path+'num_topics',num_topics) \n",
        "        \n",
        "        \n",
        "    for k in range(number_of_topics):\n",
        "      print(\"Topic \"+ str(k+1)+\" :\")\n",
        "      ids=word_topic_mat[k,:].argsort()\n",
        "      for j in range (1,10):\n",
        "        print(index_to_word[ids[-j]]+\", \",end='')\n",
        "      print('\\n')     \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n",
            "0.1\n",
            "Iter Num 1\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 2\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 3\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 4\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 5\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 6\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 7\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 8\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 9\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 10\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 11\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 12\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 13\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 14\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 15\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 16\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 17\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 18\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 19\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 20\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 21\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 22\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 23\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 24\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 25\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 26\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 27\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 28\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 29\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 30\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 31\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 32\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 33\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 34\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 35\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 36\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 37\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 38\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 39\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 40\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 41\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "Iter Num 42\n",
            "1500\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLwKCM6gWjzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1iters=26\n",
        "vals=[1,1.5,2]\n",
        "for neta in vals:\n",
        "  for alpha in vals:\n",
        "    print(alpha)\n",
        "    print(neta)\n",
        "    number_of_docs=len(vec_table[:,0])\n",
        "    number_of_terms=len(vec_table[0])\n",
        "    word_topic_mat=np.zeros(shape=(number_of_topics,number_of_terms))+neta\n",
        "    doc_topic_mat=np.zeros(shape=(number_of_docs,number_of_topics)) +alpha\n",
        "    topix=[]\n",
        "    num_topics=np.zeros(number_of_topics)+number_of_terms*neta\n",
        "    for i in range(number_of_docs):\n",
        "      topix_cur_doc=[]\n",
        "      for j in articlez[i]:\n",
        "        pz=np.divide(np.multiply(doc_topic_mat[i,:],word_topic_mat[:,j]),num_topics)\n",
        "        z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
        "        topix_cur_doc.append(z)\n",
        "        #topic=topic_mat[i][j]\n",
        "        topic=z\n",
        "        doc_topic_mat[i][topic]+=1\n",
        "        word_topic_mat[topic][j]+=1\n",
        "        num_topics[topic]+=1\n",
        "      topix.append(topix_cur_doc) \n",
        "    for i in range(iters):\n",
        "    #print(\"Iter Num \"+str(i+1))\n",
        "      new_lda()\n",
        "    for k in range(number_of_topics):\n",
        "      print(\"Topic \"+ str(k+1)+\" :\")\n",
        "      ids=word_topic_mat[k,:].argsort()\n",
        "      for j in range (1,10):\n",
        "        print(index_to_word[ids[-j]]+\", \",end='')\n",
        "      print('\\n')     \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcqypQ5XAlB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}