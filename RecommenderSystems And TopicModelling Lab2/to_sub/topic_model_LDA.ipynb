{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "-A8jOJ951B23",
    "outputId": "365c63af-a7dd-45a6-aae1-fd7920bdfced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.16.1\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "xPi4vFnK1HoA",
    "outputId": "4bb57ee9-056a-4896-d602-cd027c489cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "WkGystvt1gvT",
    "outputId": "220572ac-c110-4ee1-94fc-8ff465051a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CfD-YdT31k8O",
    "outputId": "4743e31a-7b60-4ed0-d977-499b70f21538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/RecommenderSystems And TopicModelling Lab2\n"
     ]
    }
   ],
   "source": [
    "cd drive/My Drive/RecommenderSystems And TopicModelling Lab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpcBbFwI2CAY"
   },
   "outputs": [],
   "source": [
    "path=\"data/\"\n",
    "news_path=path+\"nytimes_news_articles.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1W6Bi3ta2EHP"
   },
   "outputs": [],
   "source": [
    "stopwords=np.load(path+'stopwords.npy')\n",
    "news_df = pd.read_pickle(path+'news_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqYj-sO1BfPO"
   },
   "outputs": [],
   "source": [
    "#news_df=news_df.truncate(before=0,after=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w07-yjfV2JH6"
   },
   "outputs": [],
   "source": [
    "freq_map={}\n",
    "ps=PorterStemmer()\n",
    "for article in news_df['Article']:\n",
    "    word_list=word_tokenize(article)\n",
    "    article_map={}\n",
    "    for word in word_list:\n",
    "        word=word.replace('\\n','').replace('-',' ')\n",
    "        if len(word)>2:\n",
    "            if word.isalpha():\n",
    "                if word not in stopwords:\n",
    "                    word_s=ps.stem(word)\n",
    "                    if word_s not in stopwords:\n",
    "                      try :\n",
    "                        float(word_s.replace(',',''))\n",
    "                        continue\n",
    "                      except :  \n",
    "                        if word_s not in article_map:\n",
    "                            if word_s in freq_map:\n",
    "                                freq_map[word_s]+=1\n",
    "                            else:\n",
    "                                freq_map[word_s]=1\n",
    "                            article_map[word_s]=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8K5APzvz-4np"
   },
   "outputs": [],
   "source": [
    "word_to_index={}\n",
    "index_to_word={}\n",
    "index=0\n",
    "for key in freq_map:\n",
    "  word_to_index[key]=index\n",
    "  index_to_word[index]=key\n",
    "  index+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G_Q02JnG2Wd"
   },
   "outputs": [],
   "source": [
    "articlez=[]\n",
    "ps=PorterStemmer()\n",
    "for article in news_df['Article']:\n",
    "    this_article=[]\n",
    "    word_list=word_tokenize(article)\n",
    "    #article_map={}\n",
    "    for word in word_list:\n",
    "        word=word.replace('\\n','').replace('-',' ')\n",
    "        if len(word)>2:\n",
    "            if word.isalpha():\n",
    "                if word not in stopwords:\n",
    "                    word_s=ps.stem(word)\n",
    "                    if word_s not in stopwords:\n",
    "                      try :\n",
    "                        float(word_s.replace(',',''))\n",
    "                        continue\n",
    "                      except :  \n",
    "                        this_article.append(word_to_index[word_s])\n",
    "    articlez.append(this_article)                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYp9vuKxIg2X"
   },
   "outputs": [],
   "source": [
    "#len(articlez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9P6R0ZZ2Q85"
   },
   "outputs": [],
   "source": [
    "number_of_articles=len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvArFyooCV8S"
   },
   "outputs": [],
   "source": [
    "number_of_terms=len(freq_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xxz3DlWX2Tuu"
   },
   "outputs": [],
   "source": [
    "number_of_topics=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gw7uhm5k2VfF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkgnohJk2YiL"
   },
   "outputs": [],
   "source": [
    "vector_space_table=np.load(path+'vector_space_table2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Y6WEnHr8CQix",
    "outputId": "e9fddac9-3efe-494a-9f67-4541df45c353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8888, 68953)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_space_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fY-yBZ8rBxtB"
   },
   "outputs": [],
   "source": [
    "# for i in range(number_of_articles):\n",
    "#   article=news_df['Article'][i]\n",
    "#   word_list=word_tokenize(article)\n",
    "#   article_map={}\n",
    "#   for word in word_list:\n",
    "#     word=word.replace('\\n','').replace('-',' ')\n",
    "#     if len(word)>2:\n",
    "#       if word.isalpha():\n",
    "#         if word not in stopwords:\n",
    "#           word_s=ps.stem(word)\n",
    "#           if word_s not in stopwords:\n",
    "#             try :\n",
    "#               float(word_s.replace(',',''))\n",
    "#               continue\n",
    "#             except :  \n",
    "#               if word_s in article_map:\n",
    "#                 article_map[word_s]+=1\n",
    "#               else:\n",
    "#                 article_map[word_s]=1\n",
    "#   for word in article_map:\n",
    "#     map_index=word_to_index[word]\n",
    "#     freq_art=article_map[word]\n",
    "#     freq_total=freq_map[word]\n",
    "#     vector_space_table[i][map_index]=freq_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDZY6OXv2bQ5"
   },
   "outputs": [],
   "source": [
    "neta=0.1\n",
    "alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvdmAV6b3WIe"
   },
   "outputs": [],
   "source": [
    "iters=100\n",
    "vec_table=vector_space_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wST9zvN3XgN"
   },
   "outputs": [],
   "source": [
    "number_of_docs=len(vec_table[:,0])\n",
    "number_of_terms=len(vec_table[0])\n",
    "# word_topic_mat=np.zeros(shape=(number_of_topics,number_of_terms))+neta\n",
    "# doc_topic_mat=np.zeros(shape=(number_of_docs,number_of_topics)) +alpha\n",
    "# topix=[]\n",
    "# num_topics=np.zeros(number_of_topics)+number_of_terms*neta\n",
    "# for i in range(number_of_docs):\n",
    "#   topix_cur_doc=[]\n",
    "#   for j in articlez[i]:\n",
    "#     pz=np.divide(np.multiply(doc_topic_mat[i,:],word_topic_mat[:,j]),num_topics)\n",
    "#     z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "#     topix_cur_doc.append(z)\n",
    "#     #topic=topic_mat[i][j]\n",
    "#     topic=z\n",
    "#     doc_topic_mat[i][topic]+=1\n",
    "#     word_topic_mat[topic][j]+=1\n",
    "#     num_topics[topic]+=1\n",
    "#   topix.append(topix_cur_doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "NbD1C2CdJfvX",
    "outputId": "a2419d3c-58c1-4980-edbd-a44e033abba9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1,  0.1,  0.1,  0.1,  0.1,  4.1,  1.1, 39.1,  1.1,  0.1,  0.1,\n",
       "        0.1,  0.1, 13.1,  0.1,  4.1,  0.1,  0.1,  0.1,  0.1,  7.1,  3.1,\n",
       "        0.1, 10.1,  0.1, 72.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  1.1,\n",
       "        0.1,  0.1,  3.1,  1.1,  1.1,  0.1,  0.1,  0.1, 30.1,  0.1,  0.1,\n",
       "        0.1,  0.1,  0.1,  0.1, 79.1,  0.1,  0.1,  0.1,  0.1,  0.1, 54.1,\n",
       "       40.1,  0.1,  0.1, 18.1,  0.1,  0.1,  6.1,  0.1, 18.1,  0.1,  0.1,\n",
       "        0.1,  0.1,  0.1,  1.1])"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlpYlJMeCpEX"
   },
   "outputs": [],
   "source": [
    "def new_lda():\n",
    "  for d in range(number_of_docs):\n",
    "    if(d%1500==0 and d!=0):\n",
    "      print(d)\n",
    "    index=0\n",
    "    for w in articlez[d]:\n",
    "      z=topix[d][index]\n",
    "      word_topic_mat[z][w]-=1\n",
    "      doc_topic_mat[d][z]-=1\n",
    "      num_topics[z]-=1\n",
    "      pz = np.divide(np.multiply(doc_topic_mat[d, :], word_topic_mat[:, w]), num_topics)\n",
    "      z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "      topix[d][index]=z\n",
    "      word_topic_mat[z][w]+=1\n",
    "      doc_topic_mat[d][z]+=1\n",
    "      num_topics[z]+=1            \n",
    "      index+=1\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "og8pnVv-EOGB",
    "outputId": "eb18faf4-12d9-4d42-cd1b-5dc4ccd9cd2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       ...,\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suSyzfpYD4FH"
   },
   "outputs": [],
   "source": [
    "for i in range(26):\n",
    "  print(\"Iter Num: \"+str(i+1))\n",
    "  new_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e5Ib5Wt_3IH"
   },
   "outputs": [],
   "source": [
    "# np.save(path+'topic_mat',topic_mat)\n",
    "# np.save(path+'doc_topic',doc_topic_mat)\n",
    "# np.save(path+'word_topic',word_topic_mat) \n",
    "doc_word_map={}\n",
    "for i in range(number_of_docs):\n",
    "  doc_word_map[i]=[]\n",
    "  for j in range(number_of_terms):\n",
    "    if(vec_table[i][j]>0):\n",
    "      doc_word_map[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BH95as7H82qw"
   },
   "outputs": [],
   "source": [
    "sum_array=np.zeros(number_of_terms)\n",
    "for i in range(number_of_terms):\n",
    "  sum_array[i]=sum(vec_table[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwhRa2r79NxS"
   },
   "outputs": [],
   "source": [
    "number_of_termss=[]\n",
    "for i in range(number_of_docs):\n",
    "  number_of_termss.append(sum(vec_table[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjPAVVV14ROJ"
   },
   "outputs": [],
   "source": [
    "def lda_algo():\n",
    "  for i in range(number_of_docs):\n",
    "    if(i%1500==0 and i !=0):\n",
    "      print(i)\n",
    "    num_of_terms=number_of_termss[i]\n",
    "    for j in doc_word_map[i]:\n",
    "      p_final=0\n",
    "      init_topic=topic_mat[i][j]\n",
    "      topic_final=topic_mat[i][j]\n",
    "      for topic in range(number_of_topics):\n",
    "        #topic=topic_mat[i][j]\n",
    "        num_terms=num_of_terms-vec_table[i][j]+number_of_topics*alpha\n",
    "        num_terms_assigned=doc_topic_mat[i][topic]-vec_table[i][j]+alpha\n",
    "        if(num_terms!=0):\n",
    "          p1=num_terms_assigned/num_terms\n",
    "        else:\n",
    "          p1=0\n",
    "          \n",
    "        num_term_in_all_docs=sum_array[j]-vec_table[i][j]+number_of_terms*neta\n",
    "        num_term_with_this_topic=word_topic_mat[topic][j]-vec_table[i][j]+neta\n",
    "        if(num_term_in_all_docs!=0):\n",
    "          p2=num_term_with_this_topic/num_term_in_all_docs\n",
    "        else:    \n",
    "          p2=0\n",
    "        p_final1=p1*p2\n",
    "        if(p_final1>p_final):\n",
    "          p_final=p_final1\n",
    "          topic_final=topic\n",
    "      if init_topic!=topic_final:    \n",
    "        word_topic_mat[init_topic][j]-=  vec_table[i][j]\n",
    "        doc_topic_mat[i][init_topic]-= vec_table[i][j]\n",
    "        word_topic_mat[topic_final][j]+=  vec_table[i][j]\n",
    "        doc_topic_mat[i][topic_final]+= vec_table[i][j] \n",
    "        topic_mat[i][j]=topic_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rif0M42AErqF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ixhq0cxT_1ty"
   },
   "outputs": [],
   "source": [
    "for i in range(iters):\n",
    "  print(\"Iter Num \"+str(i+1))\n",
    "  lda_algo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHRrvQndGwD1"
   },
   "outputs": [],
   "source": [
    "word_topic_mat=np.load(path+'word_topic_mat.npy')\n",
    "doc_topic_mat=np.load(path+'doc_topic.npy')\n",
    "topix=np.load(path+'topix.npy') \n",
    "num_topics=np.load(path+'num_topics.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "d401CEpWCEU_",
    "outputId": "4bb6731d-05f8-406a-f64b-8b6d5b317274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 :\n",
      "investig, compani, report, file, case, lawyer, lawsuit, email, depart, \n",
      "\n",
      "Topic 2 :\n",
      "art, work, museum, artist, exhibit, galleri, show, new, paint, \n",
      "\n",
      "Topic 3 :\n",
      "percent, price, rose, oil, stock, fell, market, energi, cent, \n",
      "\n",
      "Topic 4 :\n",
      "travel, hotel, trip, airport, guest, includ, visit, park, book, \n",
      "\n",
      "Topic 5 :\n",
      "film, music, movi, song, album, perform, play, festiv, new, \n",
      "\n",
      "Topic 6 :\n",
      "game, point, said, play, warrior, curri, final, player, jame, \n",
      "\n",
      "Topic 7 :\n",
      "plane, fire, flight, air, egyptian, drone, aircraft, canada, fort, \n",
      "\n",
      "Topic 8 :\n",
      "anderson, cox, nativ, medit, ami, pugh, farc, duncan, wallac, \n",
      "\n",
      "Topic 9 :\n",
      "gay, transgend, right, peopl, bathroom, bar, lesbian, gender, north, \n",
      "\n",
      "Topic 10 :\n",
      "sherman, toast, ark, duckpin, ticketmast, norwegian, bowl, oslo, twain, \n",
      "\n",
      "Topic 11 :\n",
      "food, restaur, cook, recip, chef, can, make, chicken, eat, \n",
      "\n",
      "Topic 12 :\n",
      "music, opera, orchestra, compos, concert, hall, bitcoin, met, perform, \n",
      "\n",
      "Topic 13 :\n",
      "dog, puerto, rico, cat, rican, pet, anim, island, tillman, \n",
      "\n",
      "Topic 14 :\n",
      "tribun, publish, gannett, earthquak, ecuador, newspap, mcdonald, ferro, mitchel, \n",
      "\n",
      "Topic 15 :\n",
      "bank, financi, rate, tax, money, million, economi, econom, market, \n",
      "\n",
      "Topic 16 :\n",
      "million, auction, sale, sold, market, price, christi, dealer, buyer, \n",
      "\n",
      "Topic 17 :\n",
      "china, chines, beij, hong, kong, parti, taiwan, govern, foreign, \n",
      "\n",
      "Topic 18 :\n",
      "state, said, govern, unit, attack, militari, islam, american, offici, \n",
      "\n",
      "Topic 19 :\n",
      "skin, hair, bodi, product, beauti, makeup, pierc, vitamin, perfum, \n",
      "\n",
      "Topic 20 :\n",
      "said, year, will, new, one, also, can, state, peopl, \n",
      "\n",
      "Topic 21 :\n",
      "facebook, compani, appl, onlin, googl, technolog, app, media, use, \n",
      "\n",
      "Topic 22 :\n",
      "drug, health, medic, patient, hospit, care, doctor, insur, treatment, \n",
      "\n",
      "Topic 23 :\n",
      "said, offic, case, polic, court, charg, judg, lawyer, prosecutor, \n",
      "\n",
      "Topic 24 :\n",
      "trump, clinton, republican, campaign, democrat, parti, said, sander, state, \n",
      "\n",
      "Topic 25 :\n",
      "princ, baker, minneapoli, willerslev, estat, musician, paisley, sri, purpl, \n",
      "\n",
      "Topic 26 :\n",
      "game, run, said, hit, yanke, met, inning, pitch, season, \n",
      "\n",
      "Topic 27 :\n",
      "citi, new, said, york, mayor, offici, blasio, depart, peopl, \n",
      "\n",
      "Topic 28 :\n",
      "season, throne, jon, snow, game, ramsay, cersei, dragon, aravena, \n",
      "\n",
      "Topic 29 :\n",
      "new, york, graduat, univers, father, coupl, marri, receiv, mother, \n",
      "\n",
      "Topic 30 :\n",
      "design, fashion, wear, brand, dress, collect, cloth, show, men, \n",
      "\n",
      "Topic 31 :\n",
      "hastert, bedroom, room, pool, dutert, properti, philippin, bathroom, hous, \n",
      "\n",
      "Topic 32 :\n",
      "player, team, play, coach, leagu, season, game, year, said, \n",
      "\n",
      "Topic 33 :\n",
      "sex, panama, prostitut, der, buffett, worker, mossack, fonseca, offshor, \n",
      "\n",
      "Topic 34 :\n",
      "space, nasa, light, planet, earth, hole, moon, black, sun, \n",
      "\n",
      "Topic 35 :\n",
      "brazil, rousseff, presid, brazilian, parti, countri, polit, impeach, temer, \n",
      "\n",
      "Topic 36 :\n",
      "car, driver, vehicl, volkswagen, recal, test, drive, safeti, compani, \n",
      "\n",
      "Topic 37 :\n",
      "iran, iranian, rhode, nuclear, saudi, sanction, agreement, deal, tehran, \n",
      "\n",
      "Topic 38 :\n",
      "build, citi, hous, street, new, home, said, park, space, \n",
      "\n",
      "Topic 39 :\n",
      "theater, show, play, broadway, will, music, perform, award, new, \n",
      "\n",
      "Topic 40 :\n",
      "like, one, can, just, time, said, make, get, way, \n",
      "\n",
      "Topic 41 :\n",
      "said, one, famili, peopl, day, year, live, friend, time, \n",
      "\n",
      "Topic 42 :\n",
      "danc, ballet, wine, dancer, choreograph, compani, grape, vineyard, jone, \n",
      "\n",
      "Topic 43 :\n",
      "north, korea, japan, south, nuclear, obama, korean, kim, japanes, \n",
      "\n",
      "Topic 44 :\n",
      "하지만, 말했다, 것이다, falun, gong, 레이저, kasher, 때문에, leggero, \n",
      "\n",
      "Topic 45 :\n",
      "jackson, beer, hornacek, spear, drink, knick, anthoni, softe, brew, \n",
      "\n",
      "Topic 46 :\n",
      "darti, fnac, circu, conforama, peterson, wiltshir, steinhoff, hobb, nash, \n",
      "\n",
      "Topic 47 :\n",
      "compani, busi, billion, million, execut, year, invest, deal, market, \n",
      "\n",
      "Topic 48 :\n",
      "water, plant, climat, energi, environment, power, electr, product, industri, \n",
      "\n",
      "Topic 49 :\n",
      "robot, hadid, ozick, böhmermann, seneg, polanski, cashin, dakar, zaha, \n",
      "\n",
      "Topic 50 :\n",
      "team, game, player, soccer, play, goal, leagu, fan, club, \n",
      "\n",
      "Topic 51 :\n",
      "state, law, court, senat, democrat, republican, hous, bill, justic, \n",
      "\n",
      "Topic 52 :\n",
      "redston, viacom, hors, race, derbi, dauman, nyquist, amus, director, \n",
      "\n",
      "Topic 53 :\n",
      "book, women, time, articl, new, stori, write, read, wrote, \n",
      "\n",
      "Topic 54 :\n",
      "luke, kesha, opera, graham, mose, soni, haywood, abballa, widad, \n",
      "\n",
      "Topic 55 :\n",
      "rugbi, zappa, zealand, rudolph, schoning, australia, africa, dweezil, abdeslam, \n",
      "\n",
      "Topic 56 :\n",
      "show, season, televis, seri, watch, network, episod, netflix, comedi, \n",
      "\n",
      "Topic 57 :\n",
      "jewish, tiger, jew, templ, rabbi, mosqu, muslim, religi, kosher, \n",
      "\n",
      "Topic 58 :\n",
      "ali, muhammad, fight, box, ring, clay, frazier, champion, sport, \n",
      "\n",
      "Topic 59 :\n",
      "research, studi, zika, diseas, scientist, health, viru, found, brain, \n",
      "\n",
      "Topic 60 :\n",
      "india, cuban, indian, cuba, pitsiladi, kenya, german, berlin, modi, \n",
      "\n",
      "Topic 61 :\n",
      "anim, bird, speci, zoo, wildlif, gorilla, park, eleph, conserv, \n",
      "\n",
      "Topic 62 :\n",
      "school, student, univers, colleg, educ, children, class, graduat, black, \n",
      "\n",
      "Topic 63 :\n",
      "european, union, britain, europ, vote, british, parti, countri, leav, \n",
      "\n",
      "Topic 64 :\n",
      "olymp, russia, russian, athlet, sport, dope, rio, game, world, \n",
      "\n",
      "Topic 65 :\n",
      "allen, nepal, climb, politico, romeo, nepales, panton, juliet, cervant, \n",
      "\n",
      "Topic 66 :\n",
      "israel, isra, palestinian, gaza, tunnel, netanyahu, jerusalem, hama, bank, \n",
      "\n",
      "Topic 67 :\n",
      "church, cancer, christian, cloud, cathol, tumor, cell, andrew, pope, \n",
      "\n",
      "Topic 68 :\n",
      "game, goal, team, score, first, seri, second, play, said, \n",
      "\n",
      "Topic 69 :\n",
      "polic, shoot, gun, kill, orlando, shot, attack, peopl, victim, \n",
      "\n",
      "Topic 70 :\n",
      "island, sea, boat, water, beach, ship, ocean, fish, coast, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(number_of_topics):\n",
    "  print(\"Topic \"+ str(k+1)+\" :\")\n",
    "  ids=word_topic_mat[k,:].argsort()\n",
    "  for j in range (1,10):\n",
    "    print(index_to_word[ids[-j]]+\", \",end='')\n",
    "  print('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oKeuWJnHEiZx",
    "outputId": "09c2b831-e848-46ed-e04b-463180968f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.1\n",
      "Iter Num 1\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 2\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 3\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 4\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 5\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 6\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 7\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 8\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 9\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 10\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 11\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 12\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 13\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 14\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 15\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 16\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 17\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 18\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 19\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 20\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 21\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 22\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 23\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 24\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 25\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 26\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 27\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 28\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 29\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 30\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 31\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 32\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 33\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 34\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 35\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 36\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 37\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 38\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 39\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 40\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 41\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n",
      "Iter Num 42\n",
      "1500\n",
      "3000\n",
      "4500\n",
      "6000\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "niters=100\n",
    "vals=[0.1]\n",
    "for neta in vals:\n",
    "  for alpha in vals:\n",
    "    print(alpha)\n",
    "    print(neta)\n",
    "    number_of_docs=len(vec_table[:,0])\n",
    "    number_of_terms=len(vec_table[0])\n",
    "    word_topic_mat=np.zeros(shape=(number_of_topics,number_of_terms))+neta\n",
    "    doc_topic_mat=np.zeros(shape=(number_of_docs,number_of_topics)) +alpha\n",
    "    topix=[]\n",
    "    num_topics=np.zeros(number_of_topics)+number_of_terms*neta\n",
    "    for i in range(number_of_docs):\n",
    "      topix_cur_doc=[]\n",
    "      for j in articlez[i]:\n",
    "        pz=np.divide(np.multiply(doc_topic_mat[i,:],word_topic_mat[:,j]),num_topics)\n",
    "        z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "        topix_cur_doc.append(z)\n",
    "        #topic=topic_mat[i][j]\n",
    "        topic=z\n",
    "        doc_topic_mat[i][topic]+=1\n",
    "        word_topic_mat[topic][j]+=1\n",
    "        num_topics[topic]+=1\n",
    "      topix.append(topix_cur_doc) \n",
    "    for i in range(iters):\n",
    "      \n",
    "      print(\"Iter Num \"+str(i+1))\n",
    "      new_lda()\n",
    "      if(iters%10==0):\n",
    "        np.save(path+'word_topic_mat',word_topic_mat)\n",
    "        np.save(path+'doc_topic',doc_topic_mat)\n",
    "        np.save(path+'topix',topix) \n",
    "        np.save(path+'num_topics',num_topics) \n",
    "        \n",
    "        \n",
    "    for k in range(number_of_topics):\n",
    "      print(\"Topic \"+ str(k+1)+\" :\")\n",
    "      ids=word_topic_mat[k,:].argsort()\n",
    "      for j in range (1,10):\n",
    "        print(index_to_word[ids[-j]]+\", \",end='')\n",
    "      print('\\n')     \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLwKCM6gWjzX"
   },
   "outputs": [],
   "source": [
    "1iters=26\n",
    "vals=[1,1.5,2]\n",
    "for neta in vals:\n",
    "  for alpha in vals:\n",
    "    print(alpha)\n",
    "    print(neta)\n",
    "    number_of_docs=len(vec_table[:,0])\n",
    "    number_of_terms=len(vec_table[0])\n",
    "    word_topic_mat=np.zeros(shape=(number_of_topics,number_of_terms))+neta\n",
    "    doc_topic_mat=np.zeros(shape=(number_of_docs,number_of_topics)) +alpha\n",
    "    topix=[]\n",
    "    num_topics=np.zeros(number_of_topics)+number_of_terms*neta\n",
    "    for i in range(number_of_docs):\n",
    "      topix_cur_doc=[]\n",
    "      for j in articlez[i]:\n",
    "        pz=np.divide(np.multiply(doc_topic_mat[i,:],word_topic_mat[:,j]),num_topics)\n",
    "        z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "        topix_cur_doc.append(z)\n",
    "        #topic=topic_mat[i][j]\n",
    "        topic=z\n",
    "        doc_topic_mat[i][topic]+=1\n",
    "        word_topic_mat[topic][j]+=1\n",
    "        num_topics[topic]+=1\n",
    "      topix.append(topix_cur_doc) \n",
    "    for i in range(iters):\n",
    "    #print(\"Iter Num \"+str(i+1))\n",
    "      new_lda()\n",
    "    for k in range(number_of_topics):\n",
    "      print(\"Topic \"+ str(k+1)+\" :\")\n",
    "      ids=word_topic_mat[k,:].argsort()\n",
    "      for j in range (1,10):\n",
    "        print(index_to_word[ids[-j]]+\", \",end='')\n",
    "      print('\\n')     \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcqypQ5XAlB2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Copy of LDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
