{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61c3136c-2172-489d-b540-f47955feca6a",
    "_uuid": "f3b9ae521e9e18b2e3039ff28d7dd628a4b6dc2e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We will be implementing a simple search engine using nltk in python\n",
    "#With the help of TF-IDF ranking and cosine similarity we can rank the documents and get the desired output.\n",
    "#Following is the code for the same, many machine learning algorithms can be applied and the use of this search engine \n",
    "#can be extended.\n",
    "#This is the simplest search engine implementation\n",
    "\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import  gensim.models as md\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "\n",
    "#Creating the function for preprocessing the text file, the functions does the following:\n",
    "#Removing the URLs in the file\n",
    "#Removing blank lines\n",
    "#Converting the letters that were not read properly due to encoding, to be viewed properly\n",
    "#Removing stopwords from the text\n",
    "#Removing the punctuations form the text\n",
    "\n",
    "def preProcessor(textFile):\n",
    "    print('Starting pre-processing of the corpus..')\n",
    "    print('Start: Word Tokenizing')\n",
    "\n",
    "    textFilev1 = []\n",
    "    textFilev1 = [word_tokenize(sent) for sent in textFile]\n",
    "\n",
    "    print('Stop: Word Tokenizing')\n",
    "    print('Start: ASCII encoding for special characters')\n",
    "\n",
    "    textFilev2 = []\n",
    "    for sent in textFilev1:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            new_word = word.encode('ascii', 'ignore').decode('utf-8')\n",
    "            if new_word != '':\n",
    "                new_sent.append(new_word)\n",
    "        textFilev2.append(new_sent)\n",
    "\n",
    "    print('Stop: ASCII encoding for special characters')\n",
    "    print('Start: Stopwords Removal')\n",
    "\n",
    "    stopwordsFile = open('../input/stopwords/stopwords.txt')\n",
    "    stopwordsFile.seek(0)\n",
    "    stopwordsV1 = stopwordsFile.readlines()\n",
    "    stopwordsV2 = []\n",
    "    for sent in stopwordsV1:\n",
    "        sent.replace('\\n', '')\n",
    "        new_word = sent[0:len(sent) - 1]\n",
    "        stopwordsV2.append(new_word.lower())\n",
    "\n",
    "    textFilev1 = []\n",
    "    for sent in textFilev2:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            if word.lower() not in stopwordsV2:\n",
    "                new_sent.append(word.lower())\n",
    "        textFilev1.append(new_sent)\n",
    "\n",
    "    print('Stop: Stopwords Removal')\n",
    "    print('Start: Punctuation Removal')\n",
    "\n",
    "    textFilev2 = []\n",
    "    for sent in textFilev1:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            if word not in string.punctuation:\n",
    "                new_sent.append(word)\n",
    "        textFilev2.append(new_sent)\n",
    "\n",
    "    print('Stop: Punctuation Removal')\n",
    "    print('Start: Phrase Detection')\n",
    "\n",
    "    textFilev1 = []\n",
    "    common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\", \"so\", \"and\"]\n",
    "    phraseTrainer = Phrases(textFilev2, delimiter=b' ', common_terms=common_terms)\n",
    "    phraser = Phraser(phraseTrainer)\n",
    "    for article in textFilev2:\n",
    "        textFilev1.append((phraser[article]))\n",
    "\n",
    "    print('Stop: Phrase Detection')\n",
    "\n",
    "    return textFilev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "037c4974-c13f-4635-8d53-161f08aab065",
    "_uuid": "64c1712656db4e8014bc20b5c9247673d529f752",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Reading the news articles file\n",
    "nyTimesFile = open('../input/new-york-times-articles/nytimes_news_articles.txt', encoding='latin-1')\n",
    "nyTimesFile.seek(0)\n",
    "nyTimesV1 = nyTimesFile.readlines()\n",
    "nyTimesTemp = []\n",
    "nyTimesURL = []\n",
    "\n",
    "for i in range(0, len(nyTimesV1)-1):\n",
    "    if re.findall('URL', nyTimesV1[i]) == []:\n",
    "        sent = sent + nyTimesV1[i]\n",
    "        if (re.findall('URL', nyTimesV1[i+1]) != []) and (i+1 < len(nyTimesV1)):\n",
    "            nyTimesTemp.append(sent.strip())\n",
    "    else:\n",
    "        sent = ''\n",
    "        nyTimesURL.append(nyTimesV1[i])\n",
    "\n",
    "for i in range(0, len(nyTimesTemp)):\n",
    "    nyTimesTemp[i] = nyTimesTemp[i]+'articleID'+str(i)\n",
    "\n",
    "nytimes = preProcessor(nyTimesTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "188c1582-de9e-445e-8907-1880d30ddcdf",
    "_uuid": "3cce6bf99aa96ef42fb875344fb9e5ff90b4bb20",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for creating intermediate index\n",
    "def file_indexing(file):\n",
    "    fileIndex = {}\n",
    "    for index, word in enumerate(file):\n",
    "        if word in fileIndex.keys():\n",
    "            fileIndex[word].append(index)\n",
    "        else:\n",
    "            fileIndex[word] = [index]\n",
    "    return fileIndex\n",
    "\n",
    "#building final index\n",
    "def fullIndex(intIndex):\n",
    "    totalindex = {}\n",
    "    for fileName in intIndex.keys():\n",
    "        for word in intIndex[fileName].keys():\n",
    "            if word in totalindex.keys():\n",
    "                if fileName in totalindex[word].keys():\n",
    "                    totalindex[word][fileName].extend(intIndex[fileName][word][:])\n",
    "                else:\n",
    "                    totalindex[word][fileName] = intIndex[fileName][word]\n",
    "            else:\n",
    "                totalindex[word] = {fileName : intIndex[fileName][word]}\n",
    "    return totalindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d8e492ba-f17d-44f1-ada3-b2dfd6352f1f",
    "_uuid": "ee33f2fb3a7a740223bdea1134656e4cbab4497b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyTimesIndex = {}\n",
    "for sent in nytimes:\n",
    "    nyTimesIndex[' '.join(sent)] = file_indexing(sent)\n",
    "\n",
    "nyTimesIndexV1 = fullIndex(nyTimesIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43b86f2d-e8bd-45e6-ba08-6dc433ba4f3c",
    "_uuid": "e61983711485ecd129983b161bb2922f98c70e50",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions to create one word or phrase query search\n",
    "def wordSearch(word):\n",
    "    if word in nyTimesIndexV1.keys():\n",
    "        return [file for file in nyTimesIndexV1[word].keys()]\n",
    "\n",
    "\n",
    "def phraseQuery(string):\n",
    "    lists, result = [], []\n",
    "    for word in string.split():\n",
    "        lists.append(wordSearch(word))\n",
    "    setList = set(lists[0]).intersection(*lists)\n",
    "    for fileName in setList:\n",
    "        result.append(fileName)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d11cbad-8b6a-4e19-ba0c-cd51cd2821b7",
    "_uuid": "3e0401e1ab3e6fe63ae16b23789452dde6a78470",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchResult = phraseQuery('white  house')\n",
    "searchResult1 = []\n",
    "for file in wordSearch('white'):\n",
    "    searchResult1.append(file)\n",
    "for file in wordSearch('house'):\n",
    "    if file not in searchResult1:\n",
    "        searchResult1.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "edea4ab6-e92a-490c-9c17-d661c015d8f8",
    "_uuid": "2c5bd289fc0ed95afd3106d1993466d24ea574f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making use of TF-IDF ranking to rank the documents given by the searches\n",
    "#Also using similarity metrics (Cosine similarity) to get the similarity scores between both the documents \n",
    "#from both search results\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(searchResult1)\n",
    "searchResult1TFIDF = tfidf.transform(searchResult1)\n",
    "searchResultTFIDF = tfidf.transform(searchResult)\n",
    "sims = cosine_similarity(searchResult1TFIDF, searchResultTFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "63b1aa9c-569f-44a9-921c-0f40faf80118",
    "_uuid": "30ef0055fc6ec9c37ec357faecc0af9d7cf05e8d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now putting the cosine similarity results into a data frame\n",
    "#Sorting the dataframe by score and getting the most similar and appropriate 30 documents from the search results\n",
    "cosineSum = []\n",
    "for ind in range(len(sims)):\n",
    "    cosineSum.append(sum(sims[ind]))\n",
    "\n",
    "sumDF = pd.DataFrame({'score':cosineSum})\n",
    "sumDF['index'] = [i for i in range(len(cosineSum))]\n",
    "sumDF.sort_values(by='score', inplace=True, ascending=False)\n",
    "\n",
    "for ind in sumDF['index']:\n",
    "    print(nyTimesURL[int(searchResult1[ind][str(searchResult1[ind]).find('articleid')+9:])], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "560508b4-28c1-4b2a-95a1-88a38230d36d",
    "_uuid": "d1b5d9688bdc473547fa5d970360cd7551dfff47",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
